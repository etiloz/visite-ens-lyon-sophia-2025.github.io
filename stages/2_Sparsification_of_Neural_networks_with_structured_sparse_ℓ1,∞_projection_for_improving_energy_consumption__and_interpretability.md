---
layout: default
title: Sujet de stage
---
# Sparsification of Neural networks with structured sparse ℓ1,∞ projection for improving energy consumption  and interpretability

**Encadrant.e.s et équipe :** Professeur Michel Barlaud eqipe SIS  Laboratoire I3S 

**Contact :**
barlaud@i3s.unice.fr

### Description

It is well known that the impressive performance of neural networks is
achieved at the cost of a high-processing complexity. 
A first issue of Artificial Intelligence sofwre (Open AI , GEMINI, Mistral ...) using Generative Pretrained Transformers on  LLMs models is  the energy consumption.
The second issue of neural networks is the interpretability. 
We have previously developed sparse neural networks with sparsity constraint
using structured projections .

This project aims to adapt these projection based methods and foundation models
for improving complexity and interpretability on NLP Natural langage Processing datasets  (Such as Glue dataset  SST2...). 
This research project will focus in particular on Computing the trade-off between sparsity (complexity) and accuracy on NLP.
 Skills in optimization, LLMs and Transformer neural networks, master read, spoken and written English, and be adept at Python and PyTorch.

Références bibliographiques : 

Michel Barlaud and Frederic Guyard. Learning a sparse generative nonparametric
supervised autoencoder. Proceedings of the International Conference
on Acoustics, Speech and Signal Processing, Toronto, Canada, June
2021.

G. Perez, L. Condat, and M. Barlaud, “Near-linear time projection onto
the l1,infty ball application to sparse autoencoders.” IEEE International
Conference on Tools with Artificial Intelligence Washington USA, 2024.

Michel Barlaud, Guillaume Perez, and Jean-Paul Marmorat. Linear time bilevel
l1,infini projection ; application to feature selection and sparsification
of auto-encoders neural networks. http://arxiv.org/abs/2407.16293, 2024.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
Neural Information Processing Systems, 2017.

M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,
S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed, “Big
bird: Transformers for longer sequences,” Advances in Neural Information
Processing Systems, 2020

Collaboration with Ecole des Mines de Paris (PSL) and University of Louvain La Neuve (Belgium)

